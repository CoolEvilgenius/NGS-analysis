<a name="content">目录</a>

[转录组数据评估](#title)
- [数据说明](#description-for-datasets)
- [md5sum：数据完整性检查](#md5sum)
- [数据抽取](#data-extract)
- [cutadapt：数据预处理](#data-preprocess)
- [FastQC质控与统计](#fastqc)
- [rRNA过滤](#filt-rrna)
- [饱和度分析](#saturation)
- [HISAT2 mapping与统计](#hisat2-map)



<h1 name="title">转录组数据评估</h1>

<a name="description-for-datasets"><h2>数据说明 [<sup>目录</sup>](#content)</h2></a>

来自人的8个转录组样本

采用单细胞polyA建库方法，所以原始数据中很可能存在大量的rRNA序列，因此后续数据预处理中，需要去除核糖体RNA

<a name="md5sum"><h2>md5sum：数据完整性检查 [<sup>目录</sup>](#content)</h2></a>

```
$ cd /mnt/raid5/zhuqianhui/Trans_data/data_P101SC18010685-02-B14-21

$ Dir_trans='/mnt/raid5/zhuqianhui/Trans_data/data_P101SC18010685-02-B14-21';ls 2.cleandata/ | while read i;do echo "----$i----";cd $Dir_trans/2.cleandata/$i;md5sum -c *.txt;done
```

<a name="data-extract"><h2>数据抽取 [<sup>目录</sup>](#content)</h2></a>

由于原始数据测序通量过大，单端达到19G左右，测序成本过高，需要评估6G测序量是否够用

所以我们需要从原始数据中，即rawdata，对每个样本按照单端6Gbp为准，进行数据提取

先进行批量解压

```
$ ls 1.rawdata/data | while read i;
do
	gunzip -dk 1.rawdata/data/$i/*fq.gz;
done &
```

统计每个样本原始数据单端的reads数

```
$ ls 1.rawdata/data| while read i;
do
	readsNum=`wc -l 1.rawdata/data/$i/*1.fq | awk '{print $1/4}'`;
	echo -ne "$i\t$readsNum\n";
done >1.rawdata/TotalReads.txt &
```

用`FastqExtract.pl`脚本进行数据抽取

```
#!/usr/bin/perl
use strict;
use warnings;
use Getopt::Long;

# 脚本帮助文档
=head1 Description

	Thise script is used to extract a number of fastq recorders from original input fastq file

=head1 Usage

	$0 -n <totalReads> -e <base-pairs to extract> -l <readLength> -1 <input1.fastq> -2 <input2.fastq> [-o <outdir>]

=head1 Parameters

	-n	[int]	Total reads number of input.fastq
	-e	[int]	Base-pairs per end you want to extract
	-l	[int]	The reads length
	-1	[str]	Input 1st-end fastq file
	-2	[str]	Input 2nd-end fastq file
	-o	[str]	Output directory [default: current folder]

=cut

my ($totalReads,$bpNum,$length,$Input1,$Input2,$Outdir);
GetOptions(
	"n:i"=>\$totalReads,
	"e:i"=>\$bpNum,
	"l:i"=>\$length,
	"1:s"=>\$Input1,
	"2:s"=>\$Input2,
	"o:s"=>\$Outdir
	);

$Outdir=`pwd` unless (defined($Outdir));
die `pod2text $0` if ((!$totalReads) or (!$bpNum)) or (!$length) or (!$Input1) or (!$Input2);

open FQ1,"<$Input1" or die "$!\n";
open FQ2,"<$Input2" or die "$!\n";

my $Input1_basename=`basename $Input1`;
chomp $Input1_basename;
my $Input2_basename=`basename $Input2`;
chomp $Input2_basename;
open OUT1,">$Outdir/${Input1_basename}.extract" or die "$!\n";
open OUT2,">$Outdir/${Input2_basename}.extract" or die "$!\n";

my $readsRemain=$bpNum/$length;

my $remainCount=0;

while(! eof($FQ1)){
	# 读入1st-end fastq 文件的四行
	my $fq1_1=<FQ1>;
	my $fq1_2=<FQ1>;
	my $fq1_3=<FQ1>;
	my $fq1_4=<FQ1>;
	# 读入2nd-end fastq 文件的四行
	my $fq2_1=<FQ2>;
	my $fq2_2=<FQ2>;
	my $fq2_3=<FQ2>;
	my $fq2_4=<FQ2>;

	# 随机抽取
	if (rand()<$readsRemain/$totalReads){
		$remainCount++;
		print OUT1 "$fq1_1\n$fq1_2\n$fq1_3\n$fq1_4\n";
		print OUT2 "$fq2_1\n$fq2_2\n$fq2_3\n$fq2_4\n";
	}
}

print "Total reads: $totalReads\n";
print "Theorical remained reads: $readsRemain\n";
print "Practical remained reads: $remainCount\n";

close FQ1;
close FQ2;
close OUT1;
close OUT2;
```

执行数据提取

```
cat 1.rawdata/TotalReads.txt | while read i;
do
	{
	Sample=`echo $i | awk '{print $1}'`;
	TotalReads=`echo $i | awk '{print $2}'`;
	Read1=`ls 1.rawdata/data/$Sample/*1.fq`;
	Read2=`ls 1.rawdata/data/$Sample/*2.fq`;
	perl FastqExtract.pl -n $TotalReads -e 6000000000 -l 150 -1 $Read1 -2 $Read2 -o 1.rawdata/data/$Sample >1.rawdata/data/$Sample/FastqExtract.stat
	} &
done
wait
```

<a name="data-preprocess"><h2>cutadapt：数据预处理 [<sup>目录</sup>](#content)</h2></a>

使用cutadapt进行测序数据预处理，包括去接头、裁剪低质量碱基、过滤低质量reads

```
$ ls 1.rawdata/data | while read i;
do
	{
	cutadapt -j 8 -a ADAPTER -A ADAPTER -q 20,20 -m 20 -o outfile_QC_1.fastq -p outfile_QC_2.fastq infile_1.fastq infile_2.fastq
	} &
done
wait
```

<a name="fastqc"><h2>FastQC质控与统计 [<sup>目录</sup>](#content)</h2></a>

运行FastQC执行质控

```
$ cd /mnt/raid5/zhuqianhui/Trans_data/data_P101SC18010685-02-B14-21
$ mkdir -p 2.cleandata/QC
$ ls 2.cleandata/data | while read i;
do
	fastqc -t 8 -o QC 2.cleandata/data/$i/*.fq.gz
done
```

批量解压测序质量检测报告

```
$ ls *zip|while read id;do unzip $id;done
```

用`fastqc_stat.pl`脚本，统计fastqc结果

```
$ perl fastqc_stat.pl
```

`fastqc_stat.pl`脚本如下：

```
opendir (DIR, "./") or die "can't open the directory!";
@dir = readdir DIR;
foreach $file ( sort  @dir) 
{

# 跳过不需要的文件/文件夹，留下需要的文件夹
next unless -d $file;
next if $file eq '.';
next if $file eq '..';

# 提取total reads
$total_reads=  `grep '^Total' ./$file/fastqc_data.txt`;
$total_reads=(split(/\s+/,$total_reads))[2];
# 提取%GC
$GC= `grep '%GC' ./$file/fastqc_data.txt`;
$GC=(split(/\s+/,$GC))[1];
chomp $GC;

# 提取Q20，Q30
## 读入Per sequence quality scores部分的信息，保存成哈希
open FH , "<./$file/fastqc_data.txt";
while (<FH>)
    {
    next unless /#Quality/;
    while (<FH>)
        {
        @F=split;
        $hash{$F[0]}=$F[1];
        last if />>END_MODULE/;
        }
    }
## 统计Q20，Q30
$all=0;$Q20=0;$Q30=0;
$all+=$hash{$_} foreach keys %hash;
$Q20+=$hash{$_} foreach 0..20;
$Q30+=$hash{$_} foreach 0..30;
$Q20=1-$Q20/$all;
$Q30=1-$Q30/$all;
print "$file\t$total_reads\t$GC\t$Q20\t$Q30\n";
}
```

| Sample | Total Reads | GC Content | Q20 | Q30 |
|:---|:---|:---|:---|:---|
|	NP006_RRS05401_1.clean_fastqc	|	52340819	|	49	|	0.999433081	|	0.946269889	|
|	NP006_RRS05401_2.clean_fastqc	|	52340819	|	49	|	0.986299756	|	0.883258208	|
|	NP007_RRS05402_1.clean_fastqc	|	61516428	|	49	|	0.99943296	|	0.946225462	|
|	NP007_RRS05402_2.clean_fastqc	|	61516428	|	49	|	0.983493011	|	0.873715847	|
|	OMI003_RRS05395_1.clean_fastqc	|	57121434	|	49	|	0.999450826	|	0.94715594	|
|	OMI003_RRS05395_2.clean_fastqc	|	57121434	|	49	|	0.98328911	|	0.865995293	|
|	OMI024_RRS05396_1.clean_fastqc	|	59675234	|	49	|	0.999436206	|	0.951753666	|
|	OMI024_RRS05396_2.clean_fastqc	|	59675234	|	49	|	0.984757462	|	0.869313809	|
|	STA002_RRS05397_1.clean_fastqc	|	54379265	|	49	|	0.999488114	|	0.948699354	|
|	STA002_RRS05397_2.clean_fastqc	|	54379265	|	49	|	0.983382508	|	0.865130321	|
|	STA003_RRS05398_1.clean_fastqc	|	54007900	|	49	|	0.999354488	|	0.947658071	|
|	STA003_RRS05398_2.clean_fastqc	|	54007900	|	49	|	0.984410003	|	0.871314919	|
|	STA027_RRS05399_1.clean_fastqc	|	52416773	|	49	|	0.999498748	|	0.94907604	|
|	STA027_RRS05399_2.clean_fastqc	|	52416773	|	49	|	0.985606974	|	0.877793755	|
|	STA031_RRS05400_1.clean_fastqc	|	56424069	|	50	|	0.999546736	|	0.951159252	|
|	STA031_RRS05400_2.clean_fastqc	|	56424069	|	50	|	0.984513205	|	0.873813	|

<a name="saturation"><h2>饱和度分析 [<sup>目录</sup>](#content)</h2></a>

需要先下好RefSeq作为参考序列，寻找从UCSC上下载，下载地址：

`http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/refMrna.fa.gz`

用Bowtie2做好索引

```
$ nohup bowtie2-build --threads refMrna.fa refMrna.fa >refMrna.bwt_index.log 2>&1
```

将 reads 比对到 RefSeq 上

```
wd='/mnt/raid5/zhuqianhui'
dir_data='/mnt/raid5/zhuqianhui/Trans_data/data_P101SC18010685-02-B14-21'
ls $dir_data/2.cleandata/data | while read i
do
	{
	read1=`ls $dir_data/2.cleandata/data/$i/*1.clean.fq.gz`
	read2=`ls $dir_data/2.cleandata/data/$i/*2.clean.fq.gz`
	sampleName=`basename $read1 _1.clean.fq.gz`
	bowtie2 -p 8 --local -x $wd/Ref/hg19/RefSeq/refMrna.fa -1 $read1 -2 $read2 2>$dir_data/2.cleandata/Map/RefSeq_${sampleName}_map.stat | samtools sort -@ 8 -O bam -o $dir_data/2.cleandata/Map/RefSeq_${sampleName}.sorted.bam 2>$dir_data/2.cleandata/Map/RefSeq_${sampleName}_map.log
	} &
done
wait
```

用`Saturation_Depth_Stat.sh`脚本进行饱和度分析（依据测序深度）

先准备好`TotalReads.txt`文本

```
NP006_RRS05401	104681638
NP007_RRS05402	123032856
OMI003_RRS05395	114242868
OMI024_RRS05396	119350468
STA002_RRS05397	108758530
STA003_RRS05398	108015800
STA027_RRS05399	104833546
STA031_RRS05400	112848138
```


```
dir_data='/mnt/raid5/zhuqianhui/Trans_data/data_P101SC18010685-02-B14-21'

# 判断统计结果输出的文件夹是否已经创建
if [ ! -d $dir_data/2.cleandata/Stat ]
then
	mkdir $dir_data/2.cleandata/Stat
fi

cat $dir_data/2.cleandata/TotalReads.txt | while read i
do
	{
	Sample=`echo $i | awk '{print $1}'`
	Total_reads=`echo $i | awk '{print $2}'`
	export Total_reads
	Exome_length=50000000
	Reads_length=150
	Depth1_reads=$[$Exome_length/$Reads_length]
	export Depth1_reads
	Max_depth=$[$Total_reads*$Reads_length/$Exome_length]
	if [ -f $dir_data/2.cleandata/Stat/${Sample}_depth.txt ]
	then
		rm $dir_data/2.cleandata/Stat/${Sample}_depth.txt
	fi
	for ((j=5;j<=Max_depth;j+=5))
	do
		Current_depth=$j
		export Current_depth
		echo -ne "$Current_depth\t" >>$dir_data/2.cleandata/Stat/${Sample}_depth.txt
		samtools view $dir_data/2.cleandata/Map/RefSeq_${Sample}.sorted.bam | perl -ne 'chomp;next if (/^\@/);if (rand()<($ENV{"Current_depth"}*$ENV{"Depth1_reads"}/$ENV{"Total_reads"})){print "$_\n";}' | cut -f 3 | sort | uniq | wc -l >>$dir_data/2.cleandata/Stat/${Sample}_depth.txt
	done
} &
done
wait
```

将饱和度分析结果导入R中，用ggplot2绘图

先将各个样本的饱和度分析结果整合到一个文件中，共三列，为以下形式：

```
5       39867   NP006_RRS05401
10      44156   NP006_RRS05401
15      46099   NP006_RRS05401
20      47351   NP006_RRS05401
25      48345   NP006_RRS05401
30      49104   NP006_RRS05401
35      49662   NP006_RRS05401
40      50074   NP006_RRS05401
45      50498   NP006_RRS05401
50      50932   NP006_RRS05401
55      51171   NP006_RRS05401
```

```
$ ls 2.cleandata/Stat/*depth.txt | while read i;
do
	sample=`basename $i _depth.txt`;
	awk '{print $1"\t"$2"\t""'"$sample"'"}' $i;
done > 2.cleandata/Stat/Total_depth.txt
```

ggplot2绘图代码：

```
library(ggplot2)
data<-read.table("Total_depth.txt",header=T,sep="\t")
colnames(data)<-c("Depth","Gene_Count","Sample")
pdf("Saturation.pdf")
p<-ggplot(data)+geom_line(aes(x=Depth,y=Gene_Count,color=Sample))
p
dev.off()
```

<a name="hisat2-map"><h2>HISAT2 mapping与统计 [<sup>目录</sup>](#content)</h2></a>

1. 先构建HISAT2索引

	从UCSC上下载hg19的参考基因组序列

	```
	$ nohup wget -c http://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz >download.log &
	$ tar zxvf chromFa.tar.gz
	$ cat *fa >hg19.fa
	```
	
	从ENSMEBL上用下载基因组注释信息
	
	```
	$ nohup wget -c ftp://ftp.ensembl.org/pub/release-92/gtf/homo_sapiens/Homo_sapiens.GRCh38.92.gtf.gz >download.log &
	$ gunzip Homo_sapiens.GRCh38.92.gtf.gz
	```

	开始建索引

	```
	# build reference index
	##  using the python scripts included in the HISAT2 package, extract splice-site and exon information from the gene
	annotation fle
	$ extract_splice_sites.py Homo_sapiens.GRCh38.92.gtf >hg19.ss
	$ extract_exons.py Homo_sapiens.GRCh38.92.gtf >hg19.exon
	##  build a HISAT2 index
	$ nohup hisat2-build --ss hg19.ss --exon hg19.exon hg19.fa hg19_tran >hisat2_index.log 2>&1
	```
2. 批量比对

	```
	wd='/mnt/raid5/zhuqianhui'
	dir_data='/mnt/raid5/zhuqianhui/Trans_data/data_P101SC18010685-02-B14-21'
	ls $dir_data/2.cleandata/data | while read i;
	do
	    {
	    read1=`ls $dir_data/2.cleandata/data/$i/*1.clean.fq.gz`;
	    read2=`ls $dir_data/2.cleandata/data/$i/*2.clean.fq.gz`;
	    sampleName=`basename $read1 _1.clean.fq.gz`;
	    hisat2 -p 8 --dta -x $wd/Ref/hg19/hg19_trans/hg19_trans -1 $$read1 -2 $read2 2> $dir_data/2.cleandata/Map/${sampleName}_map.stat | \
	        samtools sort -@ 8 -O bam -o $dir_data/2.cleandata/Map/${sampleName}.sort.bam 1>$dir_data/2.cleandata/Map/${sampleName}_map.log 2>&1;
	    } &
	done
	wait
	```

3. Mapping结果统计

	```
	$ cd /mnt/raid5/zhuqianhui/Trans_data/data_P101SC18010685-02-B14-21
	$ ls *.stat | while read i;
	do
		sample=`basename $i _map.stat`;
		total=`head -1 $i | awk '{print $1}'`;
		uniq_concordant=`grep "aligned concordantly exactly 1 time" $i | awk '{print $1}'`;
		multi_concordant=`grep "aligned concordantly >1 times" $i | awk '{print $1}'`;
		uniq_disconcordant=`grep "aligned discordantly 1 time" $i | awk '{print $1}'`;
		uniq_mate=`grep "aligned exactly 1 time" $i | awk '{print $1}'`;
		multi_mate=`grep "aligned >1 times" $i | awk '{print $1}'`;
		
		echo -ne "$sample\t$total\t$uniq_concordant\t$multi_concordant\t$uniq_disconcordant\t$uniq_mate\t$multi_mate\n" | \
			perl -ane '
				chomp;
				$total_reads=$F[1]*2;
				$overall_reads=($F[2]+$F[3]+$F[4])*2+$F[5]+$F[6];
				$uniq_reads=($F[2]+$F[4])*2+$F[5];
				$multi_reads=$F[3]*2+$F[6];
				$overall_rate=$overall_reads/$total_reads*100;
				$uniq_rate=$uniq_reads/$total_reads*100;
				$multi_rate=$multi_reads/$total_reads*100;
				print "$F[0]\t$total_reads\t$overall_reads\t$overall_rate\t$uniq_reads\t$uniq_rate\t$multi_reads\t$multi_rate\n";';
	done
	```

| Sample | Total Reads | Overall Mapped Reads | Overall Mapped % | Unique Mapped Reads | Unique Mapped % | Multi Mapped Reads | Multi Mapped % |
|:---|:---|:---|:---|:---|:---|:---|:---|
|	NP006_RRS05401	|	104681638	|	89389222	|	85.39150104	|	85181422	|	81.37188491	|	4207800	|	4.019616124	|
|	NP007_RRS05402	|	123032856	|	101925945	|	82.84449237	|	97197878	|	79.00156199	|	4728067	|	3.84293038	|
|	OMI003_RRS05395	|	114242868	|	99637917	|	87.21587504	|	94972795	|	83.13236236	|	4665122	|	4.083512679	|
|	OMI024_RRS05396	|	119350468	|	102638016	|	85.99716258	|	97481120	|	81.67636176	|	5156896	|	4.32080082	|
|	STA002_RRS05397	|	108758530	|	94398971	|	86.79684343	|	89642428	|	82.42335383	|	4756543	|	4.373489601	|
|	STA003_RRS05398	|	108015800	|	93634082	|	86.6855423	|	89220043	|	82.59906699	|	4414039	|	4.086475312	|
|	STA027_RRS05399	|	104833546	|	90747473	|	86.56339165	|	85970602	|	82.00676719	|	4776871	|	4.556624461	|
|	STA031_RRS05400	|	112848138	|	99225446	|	87.92829705	|	95170901	|	84.33537556	|	4054545	|	3.592921489	|
